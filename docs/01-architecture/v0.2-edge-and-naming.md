# Architecture v0.2 – Edge & Naming

## 1. Purpose & Scope

Version **v0.2** introduces the **conceptual edge and naming architecture** of the Sovereign Digital Workspace.

The goal of this version is to define **how requests enter the system**, **how they are routed**, and **where trust boundaries are drawn**, without introducing implementation details or operational tooling.

This version deliberately remains **technology-agnostic** and **code-free**.

---

## 2. Role of the Edge Layer

The edge layer represents the **single, controlled entry point** into the platform.

### Responsibilities
- TLS termination
- Host- and path-based routing
- Basic request validation and rate limiting
- Forwarding requests to tenant or core services

### Explicit Non-Responsibilities
- No business logic
- No tenant authorization decisions
- No data persistence
- No identity decisions (delegated to IAM)

The edge acts as a **traffic director**, not a policy engine.

---

## 3. Separation of Concerns

The platform is conceptually split into three layers:

1. **Edge Layer** – ingress, routing, and transport security  
2. **Core Services** – shared capabilities (e.g. IAM)  
3. **Tenant Stacks** – isolated workspaces and applications  

This separation ensures that:
- Tenant isolation is enforced beyond application logic
- Core services remain minimal and replaceable
- Edge complexity does not leak into tenants

---

## 4. Naming & Domain Strategy

### Design Principles
- Predictability over flexibility
- Visual tenant separation
- DNS as a first-class isolation mechanism

### Proposed Pattern
- `iam.<domain>` – central identity service  
- `<tenant>.workspace.<domain>` – tenant workspace  
- `<service>.<tenant>.workspace.<domain>` – optional tenant-local services  

### Example
- `iam.example.com`
- `alpha.workspace.example.com`
- `chat.alpha.workspace.example.com`

This approach:
- Avoids path-based multi-tenancy
- Enables clear certificate boundaries
- Simplifies tenant onboarding and removal

---

## 5. Trust Boundaries & Data Flow

### High-Level Flow
1. User browser connects via HTTPS to the edge  
2. Edge routes the request based on hostname  
3. Tenant service validates identity via IAM  
4. Tenant service processes the request  

### Trust Assumptions
- The edge is trusted for transport security only
- Tenants trust IAM for identity assertions
- No tenant trusts another tenant

Tenant data never crosses tenant boundaries, neither at rest nor in transit.

---

## 6. Security Implications

- Compromise of a tenant does not imply compromise of other tenants
- Compromise of the edge does not grant access to tenant data without IAM credentials
- DNS and certificate management become security-relevant components

These trade-offs are accepted in favor of clarity and explainability.

---

## 7. Open Questions (for v0.3)

- Should IAM be exposed through the same edge or via a dedicated entry point?
- How granular should rate limiting be (edge vs. tenant)?
- Should internal service-to-service traffic reuse public DNS names or private addressing?

---

## 8. Resulting Decisions

This document leads to the creation of the following ADRs:
- **ADR-0003:** Central Edge Layer  
- **ADR-0004:** Subdomain-based Tenant Routing  

---

## 9. Deployment Topologies (Conceptual)

This section complements v0.2 by describing **practical deployment shapes** for the three-layer model (Edge / Core / Tenant Stacks), including alternatives and the rationale behind each option. It remains **implementation-agnostic**.

### 9.1 Default Topology (Recommended): 3 VMs (Edge / Core / Tenants)

**Shape**
- **VM1 – Edge Layer**
  - Single controlled entry point (TLS termination, routing, baseline protections)
- **VM2 – Core Services**
  - IAM (Keycloak) and its data store
- **VM3 – Tenant Stacks**
  - Per-tenant isolated application stacks (tenant = security boundary)

**Why this is the default**
- Clear separation of concerns and blast radius:
  - Edge compromise does not automatically imply tenant data exposure
  - Tenant compromise does not imply compromise of other tenants
- IAM remains stable while tenant stacks are iterated/refactored
- Refactor-friendly: tenant stacks can later be split further (e.g., a dedicated AI VM) without redesigning edge/core

**Trade-offs**
- More infrastructure overhead than a single VM
- Slightly more network/DNS coordination

---

### 9.2 Alternative A: Single VM “Monolith” (Edge + Core + Tenants on one VM)

**Shape**
- One VM runs edge routing, IAM, and all tenant stacks.

**Pros**
- Fastest initial setup and lowest cost
- Simplest network model
- Straightforward debugging (everything local)

**Cons**
- Weaker isolation story: compromise of the VM is potentially compromise of everything
- Tenant isolation becomes primarily logical (application-level), not infrastructural
- Later separation into layers can be painful (migration/refactoring cost)
- IAM and tenant workloads compete directly for resources

**When to choose**
- Very early learning sandbox or proof-of-concept with no demo-safety expectations
- Short-lived prototypes

---

### 9.3 Alternative B: Kubernetes Early (K3s/Managed) with Layers as Cluster Workloads

**Shape**
- Edge implemented via Ingress controller
- Core services as dedicated deployments (e.g., Keycloak)
- Tenants as namespaces or separate workloads (with strict policies)

**Pros**
- Strong platform narrative: GitOps, rollouts, rollbacks, standardized deployment patterns
- Scaling and lifecycle management are first-class
- Potentially strong segmentation using NetworkPolicies and namespace isolation

**Cons**
- Higher operational complexity for the current scope (2–5 tenants, 1–3 users)
- Isolation becomes policy-driven (easy to misconfigure)
- Debugging and day-2 operations require more platform expertise
- Overkill if high availability and scaling are not required

**When to choose**
- When the learning objective explicitly includes platform operations (GitOps, policy-as-code)
- When the target reference needs to resemble a “platform team” setup early

---

### 9.4 Alternative C (Not Recommended for Demo-Safe): Shared Core Data Services Across Tenants

**Shape**
- Tenants share central databases/caches/vector stores (e.g., shared Postgres/Redis/Qdrant)
- Tenant separation is implemented via schemas, prefixes, or collections

**Pros**
- Lower resource usage
- Fewer stateful components to operate and back up

**Cons**
- “Soft” isolation; cross-tenant leakage risk increases with configuration mistakes
- Harder to explain and defend in demos (“separate schema” arguments are fragile)
- Violates the principle “tenant = security boundary” in practice

**When to choose**
- Only if resource constraints dominate and strict demo-safety is not required
- Typically later, and only with robust guardrails and testing

---

### 9.5 Chosen Direction for This Project (v0.2)

For the current scope (2–5 tenants, demo-safe, browser-only, best-effort availability), the project will proceed with:

- **Default Topology: 3 VMs (Edge / Core / Tenants)**

This topology aligns best with the project principles:
- Tenant equals security boundary
- IAM-first
- Demo-safe over high availability
- Replaceability and refactorability

---

**Status:** Draft  
**Version:** v0.2 (conceptual)